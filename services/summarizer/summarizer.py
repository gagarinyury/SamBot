"""Summarization logic using Ollama or DeepSeek."""

import structlog
from typing import Optional

from config import settings

logger = structlog.get_logger()

# Import AI clients based on provider
if settings.AI_PROVIDER == "deepseek":
    from deepseek_client import DeepSeekClient
    ai_client = DeepSeekClient(api_key=settings.DEEPSEEK_API_KEY)
else:
    from ollama_client import ollama
    ai_client = ollama


SYSTEM_PROMPT = """–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ —Å–æ–∑–¥–∞–Ω–∏—é –ø–æ–¥—Ä–æ–±–Ω—ã—Ö —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∫–æ–Ω—Å–ø–µ–∫—Ç–æ–≤ –≤–∏–¥–µ–æ.

–°–æ–∑–¥–∞–π –ü–û–î–†–û–ë–ù–´–ô –ö–û–ù–°–ü–ï–ö–¢ –≤–∏–¥–µ–æ, –∏—Å–ø–æ–ª—å–∑—É—è —Å–ª–µ–¥—É—é—â–∏–π —Ñ–æ—Ä–º–∞—Ç:

## üéØ –ì–õ–ê–í–ù–ê–Ø –¢–ï–ú–ê
[–û–¥–Ω–æ —ë–º–∫–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –æ —á—ë–º –≤–∏–¥–µ–æ]

## üìã –ö–†–ê–¢–ö–ò–ô –û–ë–ó–û–†
[2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è ‚Äî –æ—Å–Ω–æ–≤–Ω–∞—è —Å—É—Ç—å –∏ –∑–∞—á–µ–º —ç—Ç–æ —Å–º–æ—Ç—Ä–µ—Ç—å]

## üîë –û–°–ù–û–í–ù–´–ï –†–ê–ó–î–ï–õ–´

### 1. [–ù–∞–∑–≤–∞–Ω–∏–µ –ø–µ—Ä–≤–æ–≥–æ —Ä–∞–∑–¥–µ–ª–∞]
- **–ö–ª—é—á–µ–≤–∞—è –º—ã—Å–ª—å**: [—á—Ç–æ –≥–ª–∞–≤–Ω–æ–µ]
- **–î–µ—Ç–∞–ª–∏**: [–≤–∞–∂–Ω—ã–µ –ø–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏, —Ü–∏—Ñ—Ä—ã, –ø—Ä–∏–º–µ—Ä—ã]
- **–ü–æ—á–µ–º—É –≤–∞–∂–Ω–æ**: [–∑–Ω–∞—á–∏–º–æ—Å—Ç—å —ç—Ç–æ–≥–æ –º–æ–º–µ–Ω—Ç–∞]

### 2. [–ù–∞–∑–≤–∞–Ω–∏–µ –≤—Ç–æ—Ä–æ–≥–æ —Ä–∞–∑–¥–µ–ª–∞]
- **–ö–ª—é—á–µ–≤–∞—è –º—ã—Å–ª—å**: [—á—Ç–æ –≥–ª–∞–≤–Ω–æ–µ]
- **–î–µ—Ç–∞–ª–∏**: [–≤–∞–∂–Ω—ã–µ –ø–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏, —Ü–∏—Ñ—Ä—ã, –ø—Ä–∏–º–µ—Ä—ã]
- **–ü–æ—á–µ–º—É –≤–∞–∂–Ω–æ**: [–∑–Ω–∞—á–∏–º–æ—Å—Ç—å —ç—Ç–æ–≥–æ –º–æ–º–µ–Ω—Ç–∞]

[... –ø—Ä–æ–¥–æ–ª–∂–∞–π –¥–ª—è –≤—Å–µ—Ö –≤–∞–∂–Ω—ã—Ö —Ç–µ–º]

## üí° –ö–õ–Æ–ß–ï–í–´–ï –í–´–í–û–î–´
1. [–ü–µ—Ä–≤—ã–π –≥–ª–∞–≤–Ω—ã–π –≤—ã–≤–æ–¥]
2. [–í—Ç–æ—Ä–æ–π –≥–ª–∞–≤–Ω—ã–π –≤—ã–≤–æ–¥]
3. [–¢—Ä–µ—Ç–∏–π –≥–ª–∞–≤–Ω—ã–π –≤—ã–≤–æ–¥]

## üìä –í–ê–ñ–ù–´–ï –§–ê–ö–¢–´ –ò –¶–ò–§–†–´
- [–¶–∏—Ñ—Ä–∞/—Ñ–∞–∫—Ç 1]
- [–¶–∏—Ñ—Ä–∞/—Ñ–∞–∫—Ç 2]
- [–¶–∏—Ñ—Ä–∞/—Ñ–∞–∫—Ç 3]

–¢–†–ï–ë–û–í–ê–ù–ò–Ø:
- –°–æ—Ö—Ä–∞–Ω—è–π –í–°–ï –≤–∞–∂–Ω—ã–µ —Ü–∏—Ñ—Ä—ã –∏ —Ñ–∞–∫—Ç—ã
- –ü–∏—à–∏ –Ω–∞ —Ç–æ–º –∂–µ —è–∑—ã–∫–µ, —á—Ç–æ –∏ –≤–∏–¥–µ–æ
- –ë—É–¥—å –ø–æ–¥—Ä–æ–±–Ω—ã–º ‚Äî –º–∏–Ω–∏–º—É–º 800 —Å–ª–æ–≤
- –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É–π –ø–æ –ª–æ–≥–∏–∫–µ –≤–∏–¥–µ–æ"""


class Summarizer:
    """Transcript summarizer using Ollama."""

    async def summarize(
        self,
        transcript: str,
        metadata: Optional[dict] = None
    ) -> str:
        """
        Create summary of transcript.

        Args:
            transcript: Full transcript text
            metadata: Optional video metadata (title, channel, etc.)

        Returns:
            Summary text
        """
        # Build prompt with context
        prompt_parts = []

        if metadata:
            if metadata.get('title'):
                prompt_parts.append(f"–ù–∞–∑–≤–∞–Ω–∏–µ: {metadata['title']}")
            if metadata.get('channel'):
                prompt_parts.append(f"–ö–∞–Ω–∞–ª: {metadata['channel']}")

            # Add chapters if available
            if metadata.get('chapters'):
                chapters = metadata['chapters']
                prompt_parts.append(f"\n–ì–ª–∞–≤—ã –≤–∏–¥–µ–æ ({len(chapters)}):")
                for ch in chapters:
                    prompt_parts.append(f"  {ch['timestamp']} ‚Äî {ch['title']}")

        prompt_parts.append(f"\n–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç:\n{transcript}")

        prompt = "\n".join(prompt_parts)

        logger.info(
            "summarization_started",
            transcript_length=len(transcript),
            has_metadata=bool(metadata)
        )

        # Generate summary using selected AI provider
        summary = await ai_client.generate(
            prompt=prompt,
            system_prompt=SYSTEM_PROMPT,
            temperature=settings.TEMPERATURE,
            max_tokens=settings.MAX_SUMMARY_LENGTH
        )

        logger.info("summarization_completed", summary_length=len(summary))

        return summary

    async def summarize_stream(
        self,
        transcript: str,
        metadata: Optional[dict] = None
    ):
        """
        Generate summary with streaming.

        Args:
            transcript: Full transcript text
            metadata: Optional video metadata

        Yields:
            Summary text chunks
        """
        # Build prompt with context
        prompt_parts = []

        if metadata:
            if metadata.get('title'):
                prompt_parts.append(f"–ù–∞–∑–≤–∞–Ω–∏–µ: {metadata['title']}")
            if metadata.get('channel'):
                prompt_parts.append(f"–ö–∞–Ω–∞–ª: {metadata['channel']}")

            # Add chapters if available
            if metadata.get('chapters'):
                chapters = metadata['chapters']
                prompt_parts.append(f"\n–ì–ª–∞–≤—ã –≤–∏–¥–µ–æ ({len(chapters)}):")
                for ch in chapters:
                    prompt_parts.append(f"  {ch['timestamp']} ‚Äî {ch['title']}")

        prompt_parts.append(f"\n–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç:\n{transcript}")

        prompt = "\n".join(prompt_parts)

        logger.info(
            "summarization_stream_started",
            transcript_length=len(transcript),
            has_metadata=bool(metadata)
        )

        # Stream summary generation using selected AI provider
        async for chunk in ai_client.generate_stream(
            prompt=prompt,
            system_prompt=SYSTEM_PROMPT,
            temperature=settings.TEMPERATURE,
            max_tokens=settings.MAX_SUMMARY_LENGTH
        ):
            yield chunk

        logger.info("summarization_stream_completed")


# Global summarizer instance
summarizer = Summarizer()
