#!/usr/bin/env python3
"""
Chunk Analyzer - –∞–Ω–∞–ª–∏–∑ –ª–µ–∫—Ü–∏–π –ø–æ —á–∞—Å—Ç—è–º —Å –ø–µ—Ä–µ–¥–∞—á–µ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.
–ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç—Å—è —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π —Å–∏—Å—Ç–µ–º–æ–π DeepSeek.
"""

import asyncio
import json
import logging
from datetime import datetime
from typing import List, Dict, Optional
from dataclasses import dataclass, asdict

from lecture_chunker import LectureChunker, ChunkInfo
from summarizers.deepseek import get_summarizer, SummaryResponse
from config import get_config

logger = logging.getLogger(__name__)

@dataclass
class ChunkAnalysisResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞ –æ–¥–Ω–æ–π —á–∞—Å—Ç–∏ –ª–µ–∫—Ü–∏–∏."""
    chunk_index: int
    time_range: str
    analysis: str
    tokens_used: int
    processing_time: float
    last_topic_timing: Optional[str] = None
    context_provided: bool = False

@dataclass
class LectureAnalysisResult:
    """–§–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞ –≤—Å–µ–π –ª–µ–∫—Ü–∏–∏."""
    video_info: Dict
    chunks_analysis: List[ChunkAnalysisResult]
    combined_summary: str
    total_tokens: int
    total_processing_time: float
    chunk_stats: Dict
    analysis_date: str

class ChunkAnalyzer:
    """
    –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –ª–µ–∫—Ü–∏–π –ø–æ —á–∞—Å—Ç—è–º —Å –ø–µ—Ä–µ–¥–∞—á–µ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –º–µ–∂–¥—É —á–∞—Å—Ç—è–º–∏.
    """
    
    def __init__(self, chunk_minutes: int = 50, overlap_minutes: int = 5):
        self.chunker = LectureChunker(chunk_minutes, overlap_minutes)
        self.config = get_config()
        
    async def analyze_lecture(self, transcript_data: Dict, video_info: Dict, 
                            target_language: str = "ru") -> LectureAnalysisResult:
        """
        –ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ª–µ–∫—Ü–∏–∏ –ø–æ —á–∞—Å—Ç—è–º.
        
        Args:
            transcript_data: –î–∞–Ω–Ω—ã–µ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∞ —Å —Å–µ–≥–º–µ–Ω—Ç–∞–º–∏
            video_info: –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –≤–∏–¥–µ–æ
            target_language: –Ø–∑—ã–∫ –∞–Ω–∞–ª–∏–∑–∞
            
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞ –≤—Å–µ–π –ª–µ–∫—Ü–∏–∏
        """
        start_time = datetime.now()
        logger.info(f"üéì –ù–∞—á–∏–Ω–∞—é –∞–Ω–∞–ª–∏–∑ –ª–µ–∫—Ü–∏–∏ –ø–æ —á–∞—Å—Ç—è–º: {video_info.get('title', 'Unknown')}")
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –ª–µ–∫—Ü–∏—é –Ω–∞ —á–∞—Å—Ç–∏
        chunks = self.chunker.split_lecture(transcript_data)
        chunk_stats = self.chunker.get_stats(chunks)
        
        logger.info(f"üìä –°–æ–∑–¥–∞–Ω–æ {len(chunks)} —á–∞—Å—Ç–µ–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥—É—é —á–∞—Å—Ç—å
        chunk_results = []
        total_tokens = 0
        
        for i, chunk in enumerate(chunks):
            logger.info(f"üîç –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é —á–∞—Å—Ç—å {i+1}/{len(chunks)}: {self.chunker.format_time(chunk.start_time)} - {self.chunker.format_time(chunk.end_time)}")
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
            content_with_context = await self._prepare_chunk_content(
                chunk, transcript_data, chunk_results
            )
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —á–∞—Å—Ç—å
            chunk_result = await self._analyze_single_chunk(
                content_with_context, chunk, target_language
            )
            
            chunk_results.append(chunk_result)
            total_tokens += chunk_result.tokens_used
            
            logger.info(f"‚úÖ –ß–∞—Å—Ç—å {i+1} –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {chunk_result.tokens_used} —Ç–æ–∫–µ–Ω–æ–≤, {chunk_result.processing_time:.1f}—Å")
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        combined_summary = await self._combine_chunk_results(chunk_results, target_language)
        
        total_processing_time = (datetime.now() - start_time).total_seconds()
        
        result = LectureAnalysisResult(
            video_info=video_info,
            chunks_analysis=chunk_results,
            combined_summary=combined_summary,
            total_tokens=total_tokens,
            total_processing_time=total_processing_time,
            chunk_stats=chunk_stats,
            analysis_date=datetime.now().isoformat()
        )
        
        logger.info(f"üèÅ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω: {total_tokens:,} —Ç–æ–∫–µ–Ω–æ–≤, {total_processing_time:.1f}—Å")
        
        return result
    
    async def _prepare_chunk_content(self, chunk: ChunkInfo, transcript_data: Dict, 
                                   previous_results: List[ChunkAnalysisResult]) -> str:
        """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç —á–∞—Å—Ç–∏ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –ø—Ä–µ–¥—ã–¥—É—â–µ–π —á–∞—Å—Ç–∏."""
        content = chunk.content
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–π —á–∞—Å—Ç–∏
        if previous_results and chunk.index > 0:
            prev_result = previous_results[-1]
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Ä–µ–º—è –Ω–∞—á–∞–ª–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            if prev_result.last_topic_timing:
                context_start_time = self._parse_timing_to_seconds(prev_result.last_topic_timing)
                
                # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
                context_content = self.chunker.get_overlap_context(
                    transcript_data, context_start_time
                )
                
                if context_content:
                    content = f"""–ö–û–ù–¢–ï–ö–°–¢ –ò–ó –ü–†–ï–î–´–î–£–©–ï–ô –ß–ê–°–¢–ò (–Ω–∞—á–∞–ª–æ —Ç–µ–º—ã: {prev_result.last_topic_timing}):
{context_content}

–û–°–ù–û–í–ù–û–ô –ê–ù–ê–õ–ò–ó –¢–ï–ö–£–©–ï–ô –ß–ê–°–¢–ò:
{chunk.content}"""
                    
                    logger.info(f"üìé –î–æ–±–∞–≤–ª–µ–Ω –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è —á–∞—Å—Ç–∏ {chunk.index + 1}")
        
        return content
    
    async def _analyze_single_chunk(self, content: str, chunk: ChunkInfo, 
                                   target_language: str) -> ChunkAnalysisResult:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –æ–¥–Ω—É —á–∞—Å—Ç—å –ª–µ–∫—Ü–∏–∏."""
        chunk_start_time = datetime.now()
        
        # –ü–æ–ª—É—á–∞–µ–º —Å—É–º–º–∞—Ä–∏–∑–∞—Ç–æ—Ä
        summarizer = get_summarizer()
        
        # –°–æ–∑–¥–∞–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è —á–∞—Å—Ç–∏ –ª–µ–∫—Ü–∏–∏
        analysis_prompt = await self._get_chunk_analysis_prompt(target_language, chunk.index + 1)
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º
        analysis_text, tokens_used = await summarizer._generate_summary_with_retry(
            content=content,
            prompt=analysis_prompt,
            target_language=target_language
        )
        
        processing_time = (datetime.now() - chunk_start_time).total_seconds()
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Ä–µ–º—è –ø–æ—Å–ª–µ–¥–Ω–µ–π —Ç–µ–º—ã –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–π —á–∞—Å—Ç–∏
        last_topic_timing = self._extract_last_topic_from_analysis(analysis_text)
        
        return ChunkAnalysisResult(
            chunk_index=chunk.index,
            time_range=f"{self.chunker.format_time(chunk.start_time)} - {self.chunker.format_time(chunk.end_time)}",
            analysis=analysis_text,
            tokens_used=tokens_used,
            processing_time=processing_time,
            last_topic_timing=last_topic_timing or chunk.last_topic_timing,
            context_provided="–ö–û–ù–¢–ï–ö–°–¢ –ò–ó –ü–†–ï–î–´–î–£–©–ï–ô –ß–ê–°–¢–ò" in content
        )
    
    async def _combine_chunk_results(self, chunk_results: List[ChunkAnalysisResult], 
                                   target_language: str) -> str:
        """–û–±—ä–µ–¥–∏–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ —á–∞—Å—Ç–µ–π –≤ —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ —Ä–µ–∑—é–º–µ."""
        logger.info("üé® –û–±—ä–µ–¥–∏–Ω—è—é —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤—Å–µ—Ö —á–∞—Å—Ç–µ–π –≤ —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ —Ä–µ–∑—é–º–µ")
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è
        combined_content = ""
        for i, result in enumerate(chunk_results):
            combined_content += f"""
–ß–ê–°–¢–¨ {i+1} ({result.time_range}):
{result.analysis}

"""
        
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è
        combining_prompt = await self._get_combining_prompt(target_language)
        
        # –ü–æ–ª—É—á–∞–µ–º —Å—É–º–º–∞—Ä–∏–∑–∞—Ç–æ—Ä –∏ –æ–±—ä–µ–¥–∏–Ω—è–µ–º
        summarizer = get_summarizer()
        final_summary, _ = await summarizer._generate_summary_with_retry(
            content=combined_content,
            prompt=combining_prompt,
            target_language=target_language
        )
        
        return final_summary
    
    async def _get_chunk_analysis_prompt(self, target_language: str, chunk_number: int) -> str:
        """–°–æ–∑–¥–∞–µ—Ç –ø—Ä–æ–º–ø—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —á–∞—Å—Ç–∏ –ª–µ–∫—Ü–∏–∏."""
        if target_language == "ru":
            return f"""–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –ß–ê–°–¢–¨ {chunk_number} –ª–µ–∫—Ü–∏–∏ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –ø–æ–¥—Ä–æ–±–Ω–æ.

–í–ê–ñ–ù–´–ï –¢–†–ï–ë–û–í–ê–ù–ò–Ø:
1. –≠—Ç–æ —á–∞—Å—Ç—å –¥–ª–∏–Ω–Ω–æ–π –ª–µ–∫—Ü–∏–∏ - –∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –ø–æ–¥—Ä–æ–±–Ω–æ –≤—Å–µ —Ç–µ–º—ã –∏ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏
2. –ï—Å–ª–∏ –µ—Å—Ç—å –ö–û–ù–¢–ï–ö–°–¢ –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–π —á–∞—Å—Ç–∏ - —É—á–∏—Ç—ã–≤–∞–π –µ–≥–æ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è
3. –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û —É–∫–∞–∑—ã–≤–∞–π –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ –¥–ª—è –≤–∞–∂–Ω—ã—Ö —Ç–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç–µ [H:MM:SS]
4. –í –∫–æ–Ω—Ü–µ –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û —É–∫–∞–∂–∏: –ü–û–°–õ–ï–î–ù–Ø–Ø_–¢–ï–ú–ê_–ù–ê–ß–ê–õ–ê–°–¨: [H:MM:SS] (–≤—Ä–µ–º—è –Ω–∞—á–∞–ª–∞ –Ω–µ–∑–∞–≤–µ—Ä—à–µ–Ω–Ω–æ–π —Ç–µ–º—ã)

–ö–û–ù–¢–ï–ù–¢ –î–õ–Ø –ê–ù–ê–õ–ò–ó–ê:
{{content}}

–°–æ–∑–¥–∞–π –¥–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —ç—Ç–æ–π —á–∞—Å—Ç–∏ –ª–µ–∫—Ü–∏–∏ —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏ –≤–∞–∂–Ω—ã—Ö –º–æ–º–µ–Ω—Ç–æ–≤.
–í–∫–ª—é—á–∏ –≤—Å–µ –∫–ª—é—á–µ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏, –ø—Ä–∏–º–µ—Ä—ã, –æ–±—ä—è—Å–Ω–µ–Ω–∏—è.
–°–æ—Ö—Ä–∞–Ω–∏ –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–π —Å—Ç–∏–ª—å –∏ –Ω–∞—É—á–Ω—É—é —Ç–æ—á–Ω–æ—Å—Ç—å."""

        else:
            return f"""Analyze PART {chunk_number} of the lecture in maximum detail.

IMPORTANT REQUIREMENTS:
1. This is part of a long lecture - analyze all topics and concepts thoroughly
2. If there's CONTEXT from previous part - use it for understanding
3. MUST include timing marks for important topics in format [H:MM:SS]  
4. At the end MUST specify: LAST_TOPIC_STARTED: [H:MM:SS] (start time of unfinished topic)

CONTENT FOR ANALYSIS:
{{content}}

Create detailed analysis of this lecture part with timing marks for important moments.
Include all key concepts, examples, explanations.
Maintain academic style and scientific accuracy."""
    
    async def _get_combining_prompt(self, target_language: str) -> str:
        """–°–æ–∑–¥–∞–µ—Ç –ø—Ä–æ–º–ø—Ç –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è —á–∞—Å—Ç–µ–π."""
        if target_language == "ru":
            return """–û–±—ä–µ–¥–∏–Ω–∏ –∞–Ω–∞–ª–∏–∑—ã –≤—Å–µ—Ö —á–∞—Å—Ç–µ–π –ª–µ–∫—Ü–∏–∏ –≤ –ï–î–ò–ù–û–ï –°–¢–†–£–ö–¢–£–†–ò–†–û–í–ê–ù–ù–û–ï –†–ï–ó–Æ–ú–ï.

–ü–û–õ–£–ß–ï–ù–ù–´–ï –ê–ù–ê–õ–ò–ó–´ –ß–ê–°–¢–ï–ô:
{content}

–¢–†–ï–ë–û–í–ê–ù–ò–Ø:
1. –°–æ–∑–¥–∞–π –¶–ï–õ–¨–ù–û–ï —Ä–µ–∑—é–º–µ –≤—Å–µ–π –ª–µ–∫—Ü–∏–∏ (–Ω–µ –ø—Ä–æ—Å—Ç–æ —Å–∫–ª–µ–π–∫—É —á–∞—Å—Ç–µ–π)
2. –°–æ—Ö—Ä–∞–Ω–∏ –≤—Å–µ –≤–∞–∂–Ω—ã–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ –∏–∑ —á–∞—Å—Ç–µ–π
3. –£–±–µ—Ä–∏ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è –º–µ–∂–¥—É —á–∞—Å—Ç—è–º–∏
4. –°–æ–∑–¥–∞–π –ª–æ–≥–∏—á–µ—Å–∫—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –≤—Å–µ–π –ª–µ–∫—Ü–∏–∏

–§–û–†–ú–ê–¢ –§–ò–ù–ê–õ–¨–ù–û–ì–û –†–ï–ó–Æ–ú–ï:
–ì–õ–ê–í–ù–û–ï: (–æ—Å–Ω–æ–≤–Ω–∞—è –∏–¥–µ—è –ª–µ–∫—Ü–∏–∏ –≤ 1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è—Ö)

–°–¢–†–£–ö–¢–£–†–ê –õ–ï–ö–¶–ò–ò:
‚Ä¢ [0:XX:XX] –¢–µ–º–∞ 1: –∫—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ
‚Ä¢ [X:XX:XX] –¢–µ–º–∞ 2: –∫—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ
‚Ä¢ [X:XX:XX] –¢–µ–º–∞ 3: –∫—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ

–ö–õ–Æ–ß–ï–í–´–ï –ú–û–ú–ï–ù–¢–´:
‚Ä¢ –ö–æ–Ω—Ü–µ–ø—Ü–∏—è 1 [–≤—Ä–µ–º—è] - –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ
‚Ä¢ –ö–æ–Ω—Ü–µ–ø—Ü–∏—è 2 [–≤—Ä–µ–º—è] - –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ
‚Ä¢ –ö–æ–Ω—Ü–µ–ø—Ü–∏—è 3 [–≤—Ä–µ–º—è] - –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ

–ü–û–î–†–û–ë–ù–´–ô –ê–ù–ê–õ–ò–ó:
[–î–µ—Ç–∞–ª—å–Ω–æ–µ –∏–∑–ª–æ–∂–µ–Ω–∏–µ –≤—Å–µ—Ö —Ç–µ–º —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏]

–í–´–í–û–î–´:
[–û—Å–Ω–æ–≤–Ω—ã–µ –∑–∞–∫–ª—é—á–µ–Ω–∏—è –ª–µ–∫—Ü–∏–∏]"""

        else:
            return """Combine all lecture part analyses into a UNIFIED STRUCTURED SUMMARY.

RECEIVED PART ANALYSES:
{content}

REQUIREMENTS:
1. Create UNIFIED summary of entire lecture (not just concatenation)
2. Preserve all important timing marks from parts
3. Remove duplications between parts
4. Create logical structure of entire lecture

FINAL SUMMARY FORMAT:
MAIN POINT: (main idea of lecture in 1-2 sentences)

LECTURE STRUCTURE:
‚Ä¢ [0:XX:XX] Topic 1: brief description
‚Ä¢ [X:XX:XX] Topic 2: brief description
‚Ä¢ [X:XX:XX] Topic 3: brief description

KEY MOMENTS:
‚Ä¢ Concept 1 [time] - explanation
‚Ä¢ Concept 2 [time] - explanation
‚Ä¢ Concept 3 [time] - explanation

DETAILED ANALYSIS:
[Detailed coverage of all topics with timing marks]

CONCLUSIONS:
[Main lecture conclusions]"""
    
    def _parse_timing_to_seconds(self, timing_str: str) -> float:
        """–ü–∞—Ä—Å–∏—Ç —Å—Ç—Ä–æ–∫—É –≤—Ä–µ–º–µ–Ω–∏ –≤ —Å–µ–∫—É–Ω–¥—ã."""
        try:
            parts = timing_str.split(':')
            if len(parts) == 3:
                hours, minutes, seconds = parts
                return int(hours) * 3600 + int(minutes) * 60 + int(seconds)
            elif len(parts) == 2:
                minutes, seconds = parts
                return int(minutes) * 60 + int(seconds)
            else:
                return float(parts[0])
        except:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –≤—Ä–µ–º—è: {timing_str}")
            return 0.0
    
    def _extract_last_topic_from_analysis(self, analysis_text: str) -> Optional[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –≤—Ä–µ–º—è –Ω–∞—á–∞–ª–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–π —Ç–µ–º—ã –∏–∑ –∞–Ω–∞–ª–∏–∑–∞."""
        import re
        
        # –ò—â–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω –ü–û–°–õ–ï–î–ù–Ø–Ø_–¢–ï–ú–ê_–ù–ê–ß–ê–õ–ê–°–¨: [–≤—Ä–µ–º—è]
        pattern = r'–ü–û–°–õ–ï–î–ù–Ø–Ø_–¢–ï–ú–ê_–ù–ê–ß–ê–õ–ê–°–¨:\s*\[?([0-9:]+)\]?'
        match = re.search(pattern, analysis_text, re.IGNORECASE)
        
        if match:
            return match.group(1)
        
        # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ - –ø–æ—Å–ª–µ–¥–Ω—è—è –≤—Ä–µ–º–µ–Ω–Ω–∞—è –º–µ—Ç–∫–∞ –≤ —Ç–µ–∫—Å—Ç–µ
        pattern = r'\[(\d{1,2}:\d{2}:\d{2})\]'
        matches = re.findall(pattern, analysis_text)
        
        if matches:
            return matches[-1]  # –ø–æ—Å–ª–µ–¥–Ω—è—è –Ω–∞–π–¥–µ–Ω–Ω–∞—è –º–µ—Ç–∫–∞
            
        return None

    def save_result(self, result: LectureAnalysisResult, output_file: Optional[str] = None) -> str:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞ –≤ —Ñ–∞–π–ª."""
        if not output_file:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_file = f"chunk_analysis_results/lecture_chunk_analysis_{timestamp}.json"
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        save_data = {
            "video_info": result.video_info,
            "analysis_info": {
                "total_chunks": len(result.chunks_analysis),
                "total_tokens": result.total_tokens,
                "total_processing_time": result.total_processing_time,
                "analysis_date": result.analysis_date,
                **result.chunk_stats
            },
            "chunks_analysis": [asdict(chunk) for chunk in result.chunks_analysis],
            "combined_summary": result.combined_summary
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(save_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {output_file}")
        return output_file