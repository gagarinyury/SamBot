#!/usr/bin/env python3
"""
–¢–µ—Å—Ç —Å–∏—Å—Ç–µ–º—ã –∞–Ω–∞–ª–∏–∑–∞ –ª–µ–∫—Ü–∏–π –ø–æ —á–∞—Å—Ç—è–º.
–¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ –¥–∞–Ω–Ω—ã—Ö –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–π –ª–µ–∫—Ü–∏–∏.
"""

import json
import asyncio
import logging
from datetime import datetime

from chunk_analyzer import ChunkAnalyzer
from lecture_chunker import LectureChunker

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

async def test_chunking_only():
    """–¢–µ—Å—Ç–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Ä–∞–∑–±–∏–≤–∫—É –Ω–∞ —á–∞—Å—Ç–∏ –±–µ–∑ –∞–Ω–∞–ª–∏–∑–∞."""
    print("üî¨ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –†–ê–ó–ë–ò–í–ö–ò –õ–ï–ö–¶–ò–ò –ù–ê –ß–ê–°–¢–ò")
    print("="*60)
    
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –ª–µ–∫—Ü–∏–∏
        print("üìÇ –ó–∞–≥—Ä—É–∂–∞—é —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç –ª–µ–∫—Ü–∏–∏...")
        with open('psychology_lecture_transcript.json', 'r', encoding='utf-8') as f:
            lecture_data = json.load(f)
        
        video_info = lecture_data['video_info']
        transcript = lecture_data['transcript']
        
        print(f"üé• –í–∏–¥–µ–æ: {video_info['title']}")
        print(f"‚è±Ô∏è –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {video_info['duration_formatted']}")
        print(f"üìù –°–∏–º–≤–æ–ª–æ–≤: {transcript['char_count']:,}")
        print()
        
        # –°–æ–∑–¥–∞–µ–º chunker –∏ —Ä–∞–∑–±–∏–≤–∞–µ–º
        chunker = LectureChunker(chunk_minutes=50, overlap_minutes=5)
        chunks = chunker.split_lecture(transcript)
        
        # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        stats = chunker.get_stats(chunks)
        
        print("üìä –†–ï–ó–£–õ–¨–¢–ê–¢ –†–ê–ó–ë–ò–í–ö–ò:")
        print(f"  –í—Å–µ–≥–æ —á–∞—Å—Ç–µ–π: {stats['total_chunks']}")
        print(f"  –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —á–∞—Å—Ç–µ–π: {stats['chunk_duration_minutes']} –º–∏–Ω—É—Ç")
        print(f"  –ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ: {stats['overlap_duration_minutes']} –º–∏–Ω—É—Ç")
        print(f"  –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∏–º–≤–æ–ª–æ–≤: {stats['total_characters']:,}")
        print(f"  –°—Ä–µ–¥–Ω–µ–µ —Å–∏–º–≤–æ–ª–æ–≤ –Ω–∞ —á–∞—Å—Ç—å: {stats['average_characters_per_chunk']:,}")
        print()
        
        print("üìù –î–ï–¢–ê–õ–ò–ó–ê–¶–ò–Ø –ü–û –ß–ê–°–¢–Ø–ú:")
        for chunk_info in stats['chunks_info']:
            print(f"  –ß–∞—Å—Ç—å {chunk_info['index'] + 1}: {chunk_info['time_range']}")
            print(f"    –°–∏–º–≤–æ–ª–æ–≤: {chunk_info['characters']:,}")
            print(f"    –ü–æ—Å–ª–µ–¥–Ω—è—è —Ç–µ–º–∞: {chunk_info['last_topic_timing']}")
            print()
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        if len(chunks) > 1:
            print("üîó –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ö–û–ù–¢–ï–ö–°–¢–ê –ú–ï–ñ–î–£ –ß–ê–°–¢–Ø–ú–ò:")
            chunk2 = chunks[1]
            if chunk2.last_topic_timing:
                context_start = chunker._parse_timing_to_seconds(chunk2.last_topic_timing) if hasattr(chunker, '_parse_timing_to_seconds') else 0
                
                # –î–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –±–µ—Ä–µ–º –≤—Ä–µ–º—è –∏–∑ –ø–µ—Ä–≤–æ–π —á–∞—Å—Ç–∏
                context_start = chunks[0].end_time - 300  # 5 –º–∏–Ω—É—Ç –Ω–∞–∑–∞–¥ –æ—Ç –∫–æ–Ω—Ü–∞ –ø–µ—Ä–≤–æ–π —á–∞—Å—Ç–∏
                
                context_content = chunker.get_overlap_context(transcript, context_start)
                print(f"  –ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è —á–∞—Å—Ç–∏ 2:")
                print(f"  –ù–∞—á–∞–ª–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {chunker.format_time(context_start)}")
                print(f"  –î–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {len(context_content)} —Å–∏–º–≤–æ–ª–æ–≤")
                print(f"  –ü—Ä–µ–≤—å—é –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {context_content[:200]}...")
                print()
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        save_data = {
            "video_info": video_info,
            "chunking_stats": stats,
            "chunks_preview": []
        }
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–µ–≤—å—é –ø–µ—Ä–≤—ã—Ö 500 —Å–∏–º–≤–æ–ª–æ–≤ –∫–∞–∂–¥–æ–π —á–∞—Å—Ç–∏
        for i, chunk in enumerate(chunks):
            save_data["chunks_preview"].append({
                "index": i,
                "time_range": f"{chunker.format_time(chunk.start_time)} - {chunker.format_time(chunk.end_time)}",
                "char_count": chunk.char_count,
                "preview": chunk.content[:500] + "..." if len(chunk.content) > 500 else chunk.content,
                "last_topic_timing": chunk.last_topic_timing
            })
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –æ—Ç–¥–µ–ª—å–Ω—É—é –ø–∞–ø–∫—É
        output_file = f"chunk_analysis_results/psychology_chunking_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(save_data, f, ensure_ascii=False, indent=2)
        
        print(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {output_file}")
        print("‚úÖ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞–∑–±–∏–≤–∫–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
        logger.error(f"Chunking test failed: {e}", exc_info=True)

async def test_full_analysis():
    """–ü–æ–ª–Ω—ã–π —Ç–µ—Å—Ç –∞–Ω–∞–ª–∏–∑–∞ –ø–æ —á–∞—Å—Ç—è–º."""
    print("üöÄ –ü–û–õ–ù–´–ô –¢–ï–°–¢ –ê–ù–ê–õ–ò–ó–ê –ü–û –ß–ê–°–¢–Ø–ú")
    print("="*60)
    
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –ª–µ–∫—Ü–∏–∏
        print("üìÇ –ó–∞–≥—Ä—É–∂–∞—é —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç –ª–µ–∫—Ü–∏–∏...")
        with open('psychology_lecture_transcript.json', 'r', encoding='utf-8') as f:
            lecture_data = json.load(f)
        
        video_info = lecture_data['video_info']
        transcript = lecture_data['transcript']
        
        print(f"üé• –í–∏–¥–µ–æ: {video_info['title']}")
        print(f"‚è±Ô∏è –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {video_info['duration_formatted']}")
        print(f"üìù –°–∏–º–≤–æ–ª–æ–≤: {transcript['char_count']:,}")
        print()
        
        # –°–æ–∑–¥–∞–µ–º –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä
        analyzer = ChunkAnalyzer(chunk_minutes=50, overlap_minutes=5)
        
        print("üéØ –ó–∞–ø—É—Å–∫–∞—é –ø–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø–æ —á–∞—Å—Ç—è–º...")
        start_time = datetime.now()
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º
        result = await analyzer.analyze_lecture(transcript, video_info, target_language="ru")
        
        total_time = (datetime.now() - start_time).total_seconds()
        
        print("="*60)
        print("üìä –†–ï–ó–£–õ–¨–¢–ê–¢ –ê–ù–ê–õ–ò–ó–ê")
        print("="*60)
        
        print(f"‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
        print(f"‚è±Ô∏è –û–±—â–µ–µ –≤—Ä–µ–º—è: {total_time:.1f} —Å–µ–∫—É–Ω–¥")
        print(f"üéØ –í—Å–µ–≥–æ —Ç–æ–∫–µ–Ω–æ–≤: {result.total_tokens:,}")
        print(f"üìä –ß–∞—Å—Ç–µ–π –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ: {len(result.chunks_analysis)}")
        print()
        
        print("üìù –ê–ù–ê–õ–ò–ó –ü–û –ß–ê–°–¢–Ø–ú:")
        for i, chunk_result in enumerate(result.chunks_analysis):
            print(f"  –ß–∞—Å—Ç—å {i+1}: {chunk_result.time_range}")
            print(f"    –¢–æ–∫–µ–Ω–æ–≤: {chunk_result.tokens_used:,}")
            print(f"    –í—Ä–µ–º—è: {chunk_result.processing_time:.1f}—Å")
            print(f"    –ö–æ–Ω—Ç–µ–∫—Å—Ç: {'–î–∞' if chunk_result.context_provided else '–ù–µ—Ç'}")
            print(f"    –ü–æ—Å–ª–µ–¥–Ω—è—è —Ç–µ–º–∞: {chunk_result.last_topic_timing}")
            print()
        
        print("üé® –§–ò–ù–ê–õ–¨–ù–û–ï –†–ï–ó–Æ–ú–ï:")
        print("-" * 60)
        print(result.combined_summary)
        print("-" * 60)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –æ—Ç–¥–µ–ª—å–Ω—É—é –ø–∞–ø–∫—É
        output_file = f"chunk_analysis_results/psychology_full_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        output_file = analyzer.save_result(result, output_file)
        print(f"üíæ –ü–æ–ª–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {output_file}")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {e}")
        logger.error(f"Full analysis test failed: {e}", exc_info=True)

async def test_single_chunk():
    """–¢–µ—Å—Ç –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤–æ–π —á–∞—Å—Ç–∏."""
    print("üî¨ –¢–ï–°–¢ –ê–ù–ê–õ–ò–ó–ê –û–î–ù–û–ô –ß–ê–°–¢–ò")
    print("="*60)
    
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –ª–µ–∫—Ü–∏–∏
        print("üìÇ –ó–∞–≥—Ä—É–∂–∞—é —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç –ª–µ–∫—Ü–∏–∏...")
        with open('psychology_lecture_transcript.json', 'r', encoding='utf-8') as f:
            lecture_data = json.load(f)
        
        video_info = lecture_data['video_info']
        transcript = lecture_data['transcript']
        
        # –°–æ–∑–¥–∞–µ–º chunker –∏ –±–µ—Ä–µ–º –ø–µ—Ä–≤—É—é —á–∞—Å—Ç—å
        chunker = LectureChunker(chunk_minutes=50, overlap_minutes=5)
        chunks = chunker.split_lecture(transcript)
        
        first_chunk = chunks[0]
        
        print(f"üé• –í–∏–¥–µ–æ: {video_info['title']}")
        print(f"üìù –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —á–∞—Å—Ç—å 1: {chunker.format_time(first_chunk.start_time)} - {chunker.format_time(first_chunk.end_time)}")
        print(f"üî§ –°–∏–º–≤–æ–ª–æ–≤ –≤ —á–∞—Å—Ç–∏: {first_chunk.char_count:,}")
        print()
        
        # –°–æ–∑–¥–∞–µ–º –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—É—é —á–∞—Å—Ç—å
        analyzer = ChunkAnalyzer()
        
        chunk_result = await analyzer._analyze_single_chunk(
            first_chunk.content, first_chunk, "ru"
        )
        
        print("üìä –†–ï–ó–£–õ–¨–¢–ê–¢ –ê–ù–ê–õ–ò–ó–ê –ü–ï–†–í–û–ô –ß–ê–°–¢–ò:")
        print(f"‚è±Ô∏è –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {chunk_result.processing_time:.1f} —Å–µ–∫—É–Ω–¥")
        print(f"üéØ –¢–æ–∫–µ–Ω–æ–≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ: {chunk_result.tokens_used:,}")
        print(f"üìç –ü–æ—Å–ª–µ–¥–Ω—è—è —Ç–µ–º–∞: {chunk_result.last_topic_timing}")
        print()
        
        print("üìù –ê–ù–ê–õ–ò–ó:")
        print("-" * 60)
        print(chunk_result.analysis)
        print("-" * 60)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        save_data = {
            "video_info": video_info,
            "chunk_info": {
                "index": first_chunk.index,
                "time_range": chunk_result.time_range,
                "char_count": first_chunk.char_count,
                "tokens_used": chunk_result.tokens_used,
                "processing_time": chunk_result.processing_time,
                "last_topic_timing": chunk_result.last_topic_timing
            },
            "analysis": chunk_result.analysis
        }
        
        output_file = f"chunk_analysis_results/psychology_single_chunk_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(save_data, f, ensure_ascii=False, indent=2)
        
        print(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {output_file}")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        logger.error(f"Single chunk test failed: {e}", exc_info=True)

async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã–±–æ—Ä–∞ —Ç–µ—Å—Ç–∞."""
    print("üß™ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –°–ò–°–¢–ï–ú–´ –ê–ù–ê–õ–ò–ó–ê –ü–û –ß–ê–°–¢–Ø–ú")
    print("="*60)
    print("1. –¢–µ—Å—Ç —Ä–∞–∑–±–∏–≤–∫–∏ –Ω–∞ —á–∞—Å—Ç–∏ (–±—ã—Å—Ç—Ä–æ)")
    print("2. –¢–µ—Å—Ç –∞–Ω–∞–ª–∏–∑–∞ –æ–¥–Ω–æ–π —á–∞—Å—Ç–∏ (—Å—Ä–µ–¥–Ω–µ)")  
    print("3. –ü–æ–ª–Ω—ã–π —Ç–µ—Å—Ç –∞–Ω–∞–ª–∏–∑–∞ –≤—Å–µ—Ö —á–∞—Å—Ç–µ–π (–º–µ–¥–ª–µ–Ω–Ω–æ, –¥–æ—Ä–æ–≥–æ)")
    print()
    
    choice = input("–í—ã–±–µ—Ä–∏—Ç–µ —Ç–µ—Å—Ç (1-3): ").strip()
    
    if choice == "1":
        await test_chunking_only()
    elif choice == "2":
        await test_single_chunk()
    elif choice == "3":
        await test_full_analysis()
    else:
        print("‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π –≤—ã–±–æ—Ä. –ó–∞–ø—É—Å–∫–∞—é —Ç–µ—Å—Ç —Ä–∞–∑–±–∏–≤–∫–∏...")
        await test_chunking_only()

if __name__ == "__main__":
    asyncio.run(main())