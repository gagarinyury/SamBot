#!/usr/bin/env python3
"""
–¢–µ—Å—Ç –∞–Ω–∞–ª–∏–∑–∞ 2-–π –∏ 3-–π —á–∞—Å—Ç–µ–π —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º.
"""

import json
import asyncio
import logging
from datetime import datetime

from chunk_analyzer import ChunkAnalyzer
from lecture_chunker import LectureChunker

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

async def test_chunk_2_with_context():
    """–¢–µ—Å—Ç –∞–Ω–∞–ª–∏–∑–∞ 2-–π —á–∞—Å—Ç–∏ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –∏–∑ 1-–π —á–∞—Å—Ç–∏."""
    print("üî¨ –¢–ï–°–¢ –ê–ù–ê–õ–ò–ó–ê 2-–ô –ß–ê–°–¢–ò –° –ö–û–ù–¢–ï–ö–°–¢–û–ú")
    print("="*60)
    
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –ª–µ–∫—Ü–∏–∏
        with open('psychology_lecture_transcript.json', 'r', encoding='utf-8') as f:
            lecture_data = json.load(f)
        
        video_info = lecture_data['video_info']
        transcript = lecture_data['transcript']
        
        # –°–æ–∑–¥–∞–µ–º chunker –∏ –ø–æ–ª—É—á–∞–µ–º —á–∞—Å—Ç–∏
        chunker = LectureChunker(chunk_minutes=50, overlap_minutes=5)
        chunks = chunker.split_lecture(transcript)
        
        chunk_2 = chunks[1]  # –≤—Ç–æ—Ä–∞—è —á–∞—Å—Ç—å
        
        print(f"üé• –í–∏–¥–µ–æ: {video_info['title']}")
        print(f"üìù –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —á–∞—Å—Ç—å 2: {chunker.format_time(chunk_2.start_time)} - {chunker.format_time(chunk_2.end_time)}")
        print(f"üî§ –°–∏–º–≤–æ–ª–æ–≤ –≤ —á–∞—Å—Ç–∏: {chunk_2.char_count:,}")
        print()
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –∏–∑ —á–∞—Å—Ç–∏ 1
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Ä–µ–º—è –ø–æ—Å–ª–µ–¥–Ω–µ–π —Ç–µ–º—ã –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞ —á–∞—Å—Ç–∏ 1
        last_topic_time_1 = "0:40:00"  # –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ —á–∞—Å—Ç–∏ 1
        context_start_seconds = 40 * 60  # 0:40:00 –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
        
        # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
        context_content = chunker.get_overlap_context(transcript, context_start_seconds)
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
        content_with_context = f"""–ö–û–ù–¢–ï–ö–°–¢ –ò–ó –ü–†–ï–î–´–î–£–©–ï–ô –ß–ê–°–¢–ò (–Ω–∞—á–∞–ª–æ —Ç–µ–º—ã: {last_topic_time_1}):
{context_content}

–û–°–ù–û–í–ù–û–ô –ê–ù–ê–õ–ò–ó –¢–ï–ö–£–©–ï–ô –ß–ê–°–¢–ò:
{chunk_2.content}"""
        
        print(f"üîó –î–æ–±–∞–≤–ª–µ–Ω –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ —á–∞—Å—Ç–∏ 1:")
        print(f"  –ù–∞—á–∞–ª–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {last_topic_time_1}")
        print(f"  –î–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {len(context_content):,} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"  –û–±—â–∞—è –¥–ª–∏–Ω–∞ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º: {len(content_with_context):,} —Å–∏–º–≤–æ–ª–æ–≤")
        print()
        
        # –°–æ–∑–¥–∞–µ–º –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —á–∞—Å—Ç—å 2 —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
        analyzer = ChunkAnalyzer()
        
        print("ü§ñ –ó–∞–ø—É—Å–∫–∞—é –∞–Ω–∞–ª–∏–∑ —á–∞—Å—Ç–∏ 2 —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º...")
        start_time = datetime.now()
        
        chunk_result = await analyzer._analyze_single_chunk(
            content_with_context, chunk_2, "ru"
        )
        
        processing_time = (datetime.now() - start_time).total_seconds()
        
        print("üìä –†–ï–ó–£–õ–¨–¢–ê–¢ –ê–ù–ê–õ–ò–ó–ê –í–¢–û–†–û–ô –ß–ê–°–¢–ò:")
        print(f"‚è±Ô∏è –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {chunk_result.processing_time:.1f} —Å–µ–∫—É–Ω–¥")
        print(f"üéØ –¢–æ–∫–µ–Ω–æ–≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ: {chunk_result.tokens_used:,}")
        print(f"üìç –ü–æ—Å–ª–µ–¥–Ω—è—è —Ç–µ–º–∞: {chunk_result.last_topic_timing}")
        print(f"üîó –ö–æ–Ω—Ç–µ–∫—Å—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω: –î–∞")
        print()
        
        print("üìù –ê–ù–ê–õ–ò–ó –ß–ê–°–¢–ò 2:")
        print("-" * 60)
        print(chunk_result.analysis)
        print("-" * 60)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        save_data = {
            "video_info": video_info,
            "chunk_info": {
                "index": 1,
                "time_range": chunk_result.time_range,
                "char_count": chunk_2.char_count,
                "tokens_used": chunk_result.tokens_used,
                "processing_time": chunk_result.processing_time,
                "last_topic_timing": chunk_result.last_topic_timing,
                "context_provided": True,
                "context_start_time": last_topic_time_1,
                "context_length": len(context_content)
            },
            "analysis": chunk_result.analysis
        }
        
        output_file = f"chunk_analysis_results/psychology_chunk2_with_context_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(save_data, f, ensure_ascii=False, indent=2)
        
        print(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {output_file}")
        
        return chunk_result.last_topic_timing  # –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –¥–ª—è —á–∞—Å—Ç–∏ 3
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        logger.error(f"Chunk 2 test failed: {e}", exc_info=True)
        return None

async def test_chunk_3_with_context(last_topic_time_2):
    """–¢–µ—Å—Ç –∞–Ω–∞–ª–∏–∑–∞ 3-–π —á–∞—Å—Ç–∏ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –∏–∑ 2-–π —á–∞—Å—Ç–∏."""
    print("\nüî¨ –¢–ï–°–¢ –ê–ù–ê–õ–ò–ó–ê 3-–ô –ß–ê–°–¢–ò –° –ö–û–ù–¢–ï–ö–°–¢–û–ú")
    print("="*60)
    
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –ª–µ–∫—Ü–∏–∏
        with open('psychology_lecture_transcript.json', 'r', encoding='utf-8') as f:
            lecture_data = json.load(f)
        
        video_info = lecture_data['video_info']
        transcript = lecture_data['transcript']
        
        # –°–æ–∑–¥–∞–µ–º chunker –∏ –ø–æ–ª—É—á–∞–µ–º —á–∞—Å—Ç–∏
        chunker = LectureChunker(chunk_minutes=50, overlap_minutes=5)
        chunks = chunker.split_lecture(transcript)
        
        chunk_3 = chunks[2]  # —Ç—Ä–µ—Ç—å—è —á–∞—Å—Ç—å
        
        print(f"üé• –í–∏–¥–µ–æ: {video_info['title']}")
        print(f"üìù –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —á–∞—Å—Ç—å 3: {chunker.format_time(chunk_3.start_time)} - {chunker.format_time(chunk_3.end_time)}")
        print(f"üî§ –°–∏–º–≤–æ–ª–æ–≤ –≤ —á–∞—Å—Ç–∏: {chunk_3.char_count:,}")
        print()
        
        if last_topic_time_2:
            # –ü–∞—Ä—Å–∏–º –≤—Ä–µ–º—è –≤ —Å–µ–∫—É–Ω–¥—ã
            time_parts = last_topic_time_2.split(':')
            context_start_seconds = int(time_parts[0]) * 3600 + int(time_parts[1]) * 60 + int(time_parts[2])
            
            # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
            context_content = chunker.get_overlap_context(transcript, context_start_seconds)
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
            content_with_context = f"""–ö–û–ù–¢–ï–ö–°–¢ –ò–ó –ü–†–ï–î–´–î–£–©–ï–ô –ß–ê–°–¢–ò (–Ω–∞—á–∞–ª–æ —Ç–µ–º—ã: {last_topic_time_2}):
{context_content}

–û–°–ù–û–í–ù–û–ô –ê–ù–ê–õ–ò–ó –¢–ï–ö–£–©–ï–ô –ß–ê–°–¢–ò:
{chunk_3.content}"""
            
            print(f"üîó –î–æ–±–∞–≤–ª–µ–Ω –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ —á–∞—Å—Ç–∏ 2:")
            print(f"  –ù–∞—á–∞–ª–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {last_topic_time_2}")
            print(f"  –î–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {len(context_content):,} —Å–∏–º–≤–æ–ª–æ–≤")
            print(f"  –û–±—â–∞—è –¥–ª–∏–Ω–∞ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º: {len(content_with_context):,} —Å–∏–º–≤–æ–ª–æ–≤")
            
        else:
            content_with_context = chunk_3.content
            print("‚ö†Ô∏è –ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ —á–∞—Å—Ç–∏ 2 –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –±–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞")
        
        print()
        
        # –°–æ–∑–¥–∞–µ–º –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —á–∞—Å—Ç—å 3
        analyzer = ChunkAnalyzer()
        
        print("ü§ñ –ó–∞–ø—É—Å–∫–∞—é –∞–Ω–∞–ª–∏–∑ —á–∞—Å—Ç–∏ 3 —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º...")
        start_time = datetime.now()
        
        chunk_result = await analyzer._analyze_single_chunk(
            content_with_context, chunk_3, "ru"
        )
        
        processing_time = (datetime.now() - start_time).total_seconds()
        
        print("üìä –†–ï–ó–£–õ–¨–¢–ê–¢ –ê–ù–ê–õ–ò–ó–ê –¢–†–ï–¢–¨–ï–ô –ß–ê–°–¢–ò:")
        print(f"‚è±Ô∏è –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {chunk_result.processing_time:.1f} —Å–µ–∫—É–Ω–¥")
        print(f"üéØ –¢–æ–∫–µ–Ω–æ–≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ: {chunk_result.tokens_used:,}")
        print(f"üìç –ü–æ—Å–ª–µ–¥–Ω—è—è —Ç–µ–º–∞: {chunk_result.last_topic_timing}")
        print(f"üîó –ö–æ–Ω—Ç–µ–∫—Å—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω: {'–î–∞' if last_topic_time_2 else '–ù–µ—Ç'}")
        print()
        
        print("üìù –ê–ù–ê–õ–ò–ó –ß–ê–°–¢–ò 3:")
        print("-" * 60)
        print(chunk_result.analysis)
        print("-" * 60)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        save_data = {
            "video_info": video_info,
            "chunk_info": {
                "index": 2,
                "time_range": chunk_result.time_range,
                "char_count": chunk_3.char_count,
                "tokens_used": chunk_result.tokens_used,
                "processing_time": chunk_result.processing_time,
                "last_topic_timing": chunk_result.last_topic_timing,
                "context_provided": bool(last_topic_time_2),
                "context_start_time": last_topic_time_2 if last_topic_time_2 else None,
                "context_length": len(context_content) if last_topic_time_2 else 0
            },
            "analysis": chunk_result.analysis
        }
        
        output_file = f"chunk_analysis_results/psychology_chunk3_with_context_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(save_data, f, ensure_ascii=False, indent=2)
        
        print(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {output_file}")
        
        return chunk_result
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        logger.error(f"Chunk 3 test failed: {e}", exc_info=True)
        return None

async def main():
    """–ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —á–∞—Å—Ç–µ–π 2 –∏ 3 —Å –ø–µ—Ä–µ–¥–∞—á–µ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞."""
    print("üöÄ –ê–ù–ê–õ–ò–ó –ß–ê–°–¢–ï–ô 2 –ò 3 –° –ü–ï–†–ï–î–ê–ß–ï–ô –ö–û–ù–¢–ï–ö–°–¢–ê")
    print("="*60)
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —á–∞—Å—Ç—å 2 —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –∏–∑ —á–∞—Å—Ç–∏ 1
    last_topic_time_2 = await test_chunk_2_with_context()
    
    if last_topic_time_2:
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —á–∞—Å—Ç—å 3 —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –∏–∑ —á–∞—Å—Ç–∏ 2
        await test_chunk_3_with_context(last_topic_time_2)
    else:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ —á–∞—Å—Ç–∏ 2 –¥–ª—è —á–∞—Å—Ç–∏ 3")

if __name__ == "__main__":
    asyncio.run(main())